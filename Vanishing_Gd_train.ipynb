{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbPJ+9IFZ7EcuTVJ8ptcq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuevu110405/Vanishing_gradient_project/blob/feature%2Ftrain/Vanishing_Gd_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6e2L6NjGOeE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.datasets import FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "utwPwQ2XI7zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "GE9H02MlMNkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7b026b-2b2b-4a6f-fef5-28a65fc51d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 199kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.98MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "train_size = int(len(train_dataset) * train_ratio)\n",
        "val_size = len(train_dataset) - train_size\n",
        "batch_size = 64\n",
        "\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle = True)\n",
        "val_loader = DataLoader(val_subset, batch_size = batch_size, shuffle = False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "print(f\"Train size: {len(train_subset)}\")\n",
        "print(f\"Validation size: {len(val_subset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "Pcgx9eGhMVw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3003c6-6ae3-43f3-bcc2-55fb076c69b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 54000\n",
            "Validation size: 6000\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preliminary\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer4(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer5(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        out = self.output(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims, hidden_dims, output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "A3tjzi6gP1OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "p0vywa6fTXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target = []\n",
        "test_predict = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_test, y_test in test_loader:\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        outputs = model(X_test)\n",
        "\n",
        "        test_predict.append(outputs.cpu())\n",
        "        test_target.append(y_test.cpu())\n",
        "\n",
        "    test_predict = torch.cat(test_predict)\n",
        "    test_target = torch.cat(test_target)\n",
        "    test_acc = (torch.argmax(test_predict, 1) == test_target).sum().item() / len(test_target)\n",
        "\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHVqtGc4he2R",
        "outputId": "c3a43469-c505-485f-a8a6-0b805c4c0534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5353\n",
            "Accuracy: 0.5353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weight increasing"
      ],
      "metadata": {
        "id": "iH2UtUh1pglz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer4(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer5(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        out = self.output(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims, hidden_dims, output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "wP-4U-f-ojNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Better Activation"
      ],
      "metadata": {
        "id": "7B6CzZcBtTtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer4(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer5(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        out = self.output(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-3\n",
        "\n",
        "model = MLP(input_dims, hidden_dims, output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "n7pzpwKjshX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Normalize Inside Network"
      ],
      "metadata": {
        "id": "Vwv2l3c3uIG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer4(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer5(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.BatchNorm1d(hidden_dims)(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        out = self.output(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims, hidden_dims, output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "T3PSDJJswKnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Skip connection"
      ],
      "metadata": {
        "id": "dAFiuCRxyPPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preliminary\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        skip = x\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = x + skip\n",
        "        x = self.layer4(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        skip = x\n",
        "        x = self.layer5(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = skip + x\n",
        "        out = self.output(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims, hidden_dims, output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "ihv-4sM-yO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train layers seperately\n"
      ],
      "metadata": {
        "id": "LH6rD1b89CHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_1layer(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(MLP_1layer, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module, nn.Linear)\n",
        "                nn.init.consant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class MLP_2layers(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(MLP_2layers, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, output_dims)\n",
        "        self.layer2 = nn.Linear(output_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std= 0.05)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BqU5CLey9BXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first = MLP_2layers(input_dims = 784, output_dims = 128).to(device)\n",
        "second = MLP_2layers(input_dims = 128, output_dims = 128).to(device)\n",
        "third = MLP_2layers(input_dims = 128, output_dims = 128).to(device)\n",
        "fourth = MLP_2layers(input_dims = 128, output_dims = 128).to(device)\n",
        "\n",
        "lr = 1e-2\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "nBngRHqbFa5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first phase"
      ],
      "metadata": {
        "id": "0tyMUhs5p1FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#Training code\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")"
      ],
      "metadata": {
        "id": "_9l9ODtyIM7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "second phase"
      ],
      "metadata": {
        "id": "SrhGYHP8p33i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8LTkYcLlpv3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "third phase"
      ],
      "metadata": {
        "id": "E-TNsUgVq4Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8xV7oCsjrJT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fourth phase"
      ],
      "metadata": {
        "id": "88HUZgyir3xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    third,\n",
        "    nn.Linear(128,10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")"
      ],
      "metadata": {
        "id": "y5vr3Ohtr3S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fifth phase"
      ],
      "metadata": {
        "id": "h6DIgVD4ufGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    third,\n",
        "    nn.Linear(128,10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")"
      ],
      "metadata": {
        "id": "MjNHW1Nquh2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sixth phase"
      ],
      "metadata": {
        "id": "qbc92sjPzZBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    parram.requires_grad = False\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in third.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    third,\n",
        "    nn.Linear(128,10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rk89OrWvzcsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "seventh phase"
      ],
      "metadata": {
        "id": "7_IgruVWyIOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in third.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = nn.Sequential(\n",
        "    first,\n",
        "    second,\n",
        "    third,\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G36U6r_UyHsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Gradient normalization"
      ],
      "metadata": {
        "id": "JySxCcAM0L3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Gradient normalization layer\n",
        "class GradientNormalization(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx,input):\n",
        "        #Forward pass : pass input unchanged\n",
        "        ctx.save_for_backward(input)\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        #Mormalize the gradient\n",
        "        mean = torch.mean(grad_output)\n",
        "        std = torch.std(grad_output)\n",
        "        grad_input = (grad_output - mean) / (std + 1e-5)\n",
        "        return grad_input\n",
        "\n",
        "#Wrapper Module for GradientNormalization\n",
        "class GradientNormalizationModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GradientNormalizationModule, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return GradientNormalization.apply(x)\n"
      ],
      "metadata": {
        "id": "NBZGfH6E0RDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}